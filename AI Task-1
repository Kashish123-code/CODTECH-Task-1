# Importing necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_splitrÌ¥
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import seaborn as sns

# Sample dataset (Breast Cancer dataset from sklearn)
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()

# Creating a DataFrame
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Defining the models
models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC()
}

# Initializing a dictionary to store the results
results = {}

# Function to evaluate a model
def evaluate_model(name, model, X_train, X_test, y_train, y_test):
    # Train the model
    model.fit(X_train, y_train)

    # Predicting the test set results
    y_pred = model.predict(X_test)

    # Calculating evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Printing the metrics
    print(f'\n{name} Model:')
    print(f'Accuracy: {accuracy:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1-Score: {f1:.4f}')

    # Storing the results
    results[name] = [accuracy, precision, recall, f1, cm]

# Evaluating all models
for name, model in models.items():
    evaluate_model(name, model, X_train_scaled, X_test_scaled, y_train, y_test)

# Comparing models visually using bar charts
def plot_comparison(results):
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']

    fig, axs = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('Model Comparison')

    # Creating subplots for each metric
    for i, metric in enumerate(metrics):
        ax = axs[i // 2, i % 2]
        scores = [results[model][i] for model in models]
        ax.bar(models.keys(), scores, color=['blue', 'green', 'orange'])
        ax.set_title(metric)
        ax.set_ylim([0, 1])

    plt.tight_layout()
    plt.show()

# Plot the comparison
plot_comparison(results)

# Plotting confusion matrices
def plot_confusion_matrices(results):
    fig, axs = plt.subplots(1, 3, figsize=(18, 6))
    fig.suptitle('Confusion Matrices')

    for i, (model, cm) in enumerate(results.items()):
        sns.heatmap(cm[4], annot=True, fmt='d', cmap='Blues', ax=axs[i])
        axs[i].set_title(model)

    plt.tight_layout()
    plt.show()

# Plot the confusion matrices
plot_confusion_matrices(results)
